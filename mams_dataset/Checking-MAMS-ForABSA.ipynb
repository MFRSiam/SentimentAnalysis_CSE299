{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In This Notebook We Explore How The Mams-For-ABSA is Doing Sentiment Analysis And Try To Find out The Output Variable For this Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Entry Code of Traning The Model Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "# Import Custom Training Class From The Absa Dataset\n",
    "from train.train import train\n",
    "\n",
    "# Load The Configuration From The Config YAML File\n",
    "config = yaml.safe_load(open('config.yml'))\n",
    "\n",
    "# Check The Mode There is 2 Options for the mode Term Based And Category Based\n",
    "mode = config['mode']\n",
    "\n",
    "# Enable GPU For Traning The Model \n",
    "# This Code Forces The Pytorch Library To Use only 1 GPU \n",
    "# As Many systems have multiple gpus including my laptop\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['gpu'])\n",
    "# This Basically Pulls Out The GPU Value From The Config Value\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "#The Main Traning Code\n",
    "## train(config) -> This is The Main Function Explained Later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now If We Look Into This Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from train import make_aspect_term_model, make_aspect_category_model\n",
    "from train.make_data import make_term_data, make_category_data\n",
    "from train.make_optimizer import make_optimizer\n",
    "from train.eval import eval\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from src.module.utils.loss import CapsuleLoss\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    # Check Which Mode The Model's Is in\n",
    "    mode = config['mode']\n",
    "    # Depending on the model type, it creates the model and loads the training and validation data.\n",
    "    if mode == 'term':\n",
    "        # The make_model and make_term_data function is explored later\n",
    "        model = make_aspect_term_model.make_model(config)\n",
    "        train_loader, val_loader = make_term_data(config)\n",
    "    else:\n",
    "        # The makde_model and the make_category_data is explored later\n",
    "        model = make_aspect_category_model.make_model(config)\n",
    "        train_loader, val_loader = make_category_data(config)\n",
    "    # The model is moved to the GPU using model.cuda().\n",
    "    model = model.cuda()\n",
    "    \n",
    "    base_path = config['base_path']\n",
    "    # The model's path is set using model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type']).\n",
    "    model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    # The index2word dictionary is loaded from disk using with\n",
    "    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'rb') as handle:\n",
    "        index2word = pickle.load(handle)\n",
    "    \n",
    "    # The CapsuleLoss function is instantiated using criterion = CapsuleLoss().\n",
    "    criterion = CapsuleLoss()\n",
    "    # An optimizer is created for the model using optimizer = make_optimizer(config, model).\n",
    "    optimizer = make_optimizer(config, model)\n",
    "    # The max_val_accuracy and min_val_loss variables are set to 0 and 100, respectively.\n",
    "    max_val_accuracy = 0\n",
    "    min_val_loss = 100\n",
    "    global_step = 0\n",
    "    config = config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]\n",
    "    # A loop is run for config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['num_epoches'] epochs. \n",
    "    # For each epoch, the training data is looped over and the model is trained using \n",
    "    # stochastic gradient descent. \n",
    "    # After each epoch, the model is evaluated on the validation data. \n",
    "    # The training and validation loss and accuracy are printed.\n",
    "    for epoch in range(config['num_epoches']):\n",
    "        # Initialize the total loss for the epoch to 0.\n",
    "        # Initialize the total number of samples seen in the epoch to 0.\n",
    "        # Initialize the total number of correct predictions in the epoch to 0.\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_samples = 0\n",
    "        start = time.time() #  Start a timer to measure the time taken for the epoch.\n",
    "        # This loop iterates over the batches in the training data.\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #  Increment the global step count by 1.\n",
    "            global_step += 1\n",
    "            # Set the model to training mode.\n",
    "            model.train()\n",
    "            # Extract the inputs and labels for the current batch.\n",
    "            input0, input1, label = data\n",
    "            # Move the inputs and labels to the GPU if available.\n",
    "            input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()\n",
    "            # Reset the gradients to zero.\n",
    "            optimizer.zero_grad()\n",
    "            #  Forward pass the inputs through the model to get the predictions.\n",
    "            logit = model(input0, input1)\n",
    "            # Calculate the loss based on the predictions and labels.\n",
    "            loss = criterion(logit, label)\n",
    "            # Get the size of the current batch.\n",
    "            batch_size = input0.size(0)\n",
    "            # Update the total loss for the epoch.\n",
    "            total_loss += batch_size * loss.item()\n",
    "            #  Update the total number of samples seen in the epoch.\n",
    "            total_samples += batch_size\n",
    "            # Get the predicted labels based on the highest predicted probability.\n",
    "            pred = logit.argmax(dim=1)\n",
    "            # Update the total number of correct predictions in the epoch.\n",
    "            correct_samples += (label == pred).long().sum().item()\n",
    "            # Backpropagate the loss to compute the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the gradients to avoid exploding gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            # Update the model parameters based on the computed gradients.\n",
    "            optimizer.step()\n",
    "            # Every 10 batches (except for the first batch), perform the following steps:\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                # Calculate the average loss for the epoch so far.\n",
    "                train_loss = total_loss / total_samples\n",
    "                #  Calculate the accuracy for the epoch so far.\n",
    "                train_accuracy = correct_samples / total_samples\n",
    "                # Reset the total loss for the epoch to 0.\n",
    "                # Reset the total number of samples seen in the epoch to 0.\n",
    "                # Reset the total number of correct predictions in the epoch to 0.\n",
    "                total_loss = 0\n",
    "                total_samples = 0\n",
    "                correct_samples = 0\n",
    "                # Perform validation on the validation dataset and get the validation accuracy and loss.\n",
    "                val_accuracy, val_loss = eval(model, val_loader, criterion)\n",
    "                # Print out the training and validation loss and accuracy for the current batch.\n",
    "                print('[epoch %2d] [step %3d] train_loss: %.4f train_acc: %.4f val_loss: %.4f val_acc: %.4f'\n",
    "                      % (epoch, i, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "                if val_accuracy > max_val_accuracy:\n",
    "                    max_val_accuracy = val_accuracy\n",
    "                    # torch.save(aspect_term_model.state_dict(), model_path)\n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss = val_loss\n",
    "                    if epoch > 0:\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "        end = time.time()\n",
    "        print('time: %.4fs' % (end - start))\n",
    "    print('max_val_accuracy:', max_val_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now Trying To Train The Model Using CUDNN Backend Allowing GPU Based Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  0] [step  10] train_loss: 0.3386 train_acc: 0.4730 val_loss: 0.4271 val_acc: 0.2714\n",
      "[epoch  0] [step  20] train_loss: 0.3016 train_acc: 0.6484 val_loss: 0.4574 val_acc: 0.2725\n",
      "[epoch  0] [step  30] train_loss: 0.2824 train_acc: 0.6391 val_loss: 0.5017 val_acc: 0.2736\n",
      "time: 3.2520s\n",
      "[epoch  1] [step  10] train_loss: 0.2570 train_acc: 0.6449 val_loss: 0.5577 val_acc: 0.2736\n",
      "[epoch  1] [step  20] train_loss: 0.2413 train_acc: 0.6328 val_loss: 0.5765 val_acc: 0.2736\n",
      "[epoch  1] [step  30] train_loss: 0.2296 train_acc: 0.6484 val_loss: 0.5822 val_acc: 0.2714\n",
      "time: 2.3053s\n",
      "[epoch  2] [step  10] train_loss: 0.2232 train_acc: 0.6293 val_loss: 0.5436 val_acc: 0.2804\n",
      "[epoch  2] [step  20] train_loss: 0.2028 train_acc: 0.6937 val_loss: 0.5834 val_acc: 0.2815\n",
      "[epoch  2] [step  30] train_loss: 0.1989 train_acc: 0.6969 val_loss: 0.5285 val_acc: 0.3277\n",
      "time: 2.2485s\n",
      "[epoch  3] [step  10] train_loss: 0.1798 train_acc: 0.7287 val_loss: 0.5286 val_acc: 0.3446\n",
      "[epoch  3] [step  20] train_loss: 0.1609 train_acc: 0.7781 val_loss: 0.5506 val_acc: 0.3345\n",
      "[epoch  3] [step  30] train_loss: 0.1684 train_acc: 0.7531 val_loss: 0.6171 val_acc: 0.3255\n",
      "time: 2.6152s\n",
      "[epoch  4] [step  10] train_loss: 0.1481 train_acc: 0.7997 val_loss: 0.6063 val_acc: 0.3243\n",
      "[epoch  4] [step  20] train_loss: 0.1295 train_acc: 0.8219 val_loss: 0.6014 val_acc: 0.3401\n",
      "[epoch  4] [step  30] train_loss: 0.1439 train_acc: 0.7937 val_loss: 0.6259 val_acc: 0.3345\n",
      "time: 2.4411s\n",
      "[epoch  5] [step  10] train_loss: 0.1133 train_acc: 0.8480 val_loss: 0.6282 val_acc: 0.3412\n",
      "[epoch  5] [step  20] train_loss: 0.1154 train_acc: 0.8344 val_loss: 0.6423 val_acc: 0.3367\n",
      "[epoch  5] [step  30] train_loss: 0.1274 train_acc: 0.8172 val_loss: 0.6480 val_acc: 0.3480\n",
      "time: 2.1576s\n",
      "[epoch  6] [step  10] train_loss: 0.1029 train_acc: 0.8636 val_loss: 0.6448 val_acc: 0.3390\n",
      "[epoch  6] [step  20] train_loss: 0.1065 train_acc: 0.8516 val_loss: 0.6555 val_acc: 0.3367\n",
      "[epoch  6] [step  30] train_loss: 0.1052 train_acc: 0.8531 val_loss: 0.6765 val_acc: 0.3367\n",
      "time: 2.3364s\n",
      "[epoch  7] [step  10] train_loss: 0.0792 train_acc: 0.8991 val_loss: 0.6791 val_acc: 0.3356\n",
      "[epoch  7] [step  20] train_loss: 0.0951 train_acc: 0.8641 val_loss: 0.6872 val_acc: 0.3390\n",
      "[epoch  7] [step  30] train_loss: 0.0923 train_acc: 0.8656 val_loss: 0.6876 val_acc: 0.3401\n",
      "time: 2.4704s\n",
      "[epoch  8] [step  10] train_loss: 0.0888 train_acc: 0.8878 val_loss: 0.6933 val_acc: 0.3356\n",
      "[epoch  8] [step  20] train_loss: 0.0793 train_acc: 0.8891 val_loss: 0.6817 val_acc: 0.3491\n",
      "[epoch  8] [step  30] train_loss: 0.0848 train_acc: 0.8797 val_loss: 0.6723 val_acc: 0.3547\n",
      "time: 2.6844s\n",
      "[epoch  9] [step  10] train_loss: 0.0785 train_acc: 0.8906 val_loss: 0.6778 val_acc: 0.3514\n",
      "[epoch  9] [step  20] train_loss: 0.0815 train_acc: 0.8875 val_loss: 0.6936 val_acc: 0.3412\n",
      "[epoch  9] [step  30] train_loss: 0.0819 train_acc: 0.8938 val_loss: 0.6863 val_acc: 0.3491\n",
      "time: 2.2551s\n",
      "[epoch 10] [step  10] train_loss: 0.0712 train_acc: 0.9006 val_loss: 0.6890 val_acc: 0.3514\n",
      "[epoch 10] [step  20] train_loss: 0.0711 train_acc: 0.8938 val_loss: 0.6919 val_acc: 0.3435\n",
      "[epoch 10] [step  30] train_loss: 0.0686 train_acc: 0.9141 val_loss: 0.6955 val_acc: 0.3480\n",
      "time: 2.2447s\n",
      "[epoch 11] [step  10] train_loss: 0.0574 train_acc: 0.9219 val_loss: 0.7026 val_acc: 0.3390\n",
      "[epoch 11] [step  20] train_loss: 0.0655 train_acc: 0.9031 val_loss: 0.7133 val_acc: 0.3457\n",
      "[epoch 11] [step  30] train_loss: 0.0713 train_acc: 0.9000 val_loss: 0.7036 val_acc: 0.3423\n",
      "time: 2.1698s\n",
      "[epoch 12] [step  10] train_loss: 0.0628 train_acc: 0.9105 val_loss: 0.7038 val_acc: 0.3502\n",
      "[epoch 12] [step  20] train_loss: 0.0467 train_acc: 0.9422 val_loss: 0.7086 val_acc: 0.3423\n",
      "[epoch 12] [step  30] train_loss: 0.0574 train_acc: 0.9219 val_loss: 0.7108 val_acc: 0.3446\n",
      "time: 2.0104s\n",
      "[epoch 13] [step  10] train_loss: 0.0486 train_acc: 0.9403 val_loss: 0.7097 val_acc: 0.3525\n",
      "[epoch 13] [step  20] train_loss: 0.0682 train_acc: 0.9125 val_loss: 0.7151 val_acc: 0.3491\n",
      "[epoch 13] [step  30] train_loss: 0.0490 train_acc: 0.9344 val_loss: 0.7166 val_acc: 0.3525\n",
      "time: 2.3104s\n",
      "[epoch 14] [step  10] train_loss: 0.0444 train_acc: 0.9489 val_loss: 0.7243 val_acc: 0.3345\n",
      "[epoch 14] [step  20] train_loss: 0.0463 train_acc: 0.9422 val_loss: 0.7227 val_acc: 0.3390\n",
      "[epoch 14] [step  30] train_loss: 0.0434 train_acc: 0.9437 val_loss: 0.7344 val_acc: 0.3423\n",
      "time: 2.2765s\n",
      "[epoch 15] [step  10] train_loss: 0.0481 train_acc: 0.9304 val_loss: 0.7279 val_acc: 0.3412\n",
      "[epoch 15] [step  20] train_loss: 0.0409 train_acc: 0.9484 val_loss: 0.7257 val_acc: 0.3412\n",
      "[epoch 15] [step  30] train_loss: 0.0476 train_acc: 0.9375 val_loss: 0.7180 val_acc: 0.3468\n",
      "time: 2.1281s\n",
      "[epoch 16] [step  10] train_loss: 0.0401 train_acc: 0.9432 val_loss: 0.7463 val_acc: 0.3322\n",
      "[epoch 16] [step  20] train_loss: 0.0461 train_acc: 0.9406 val_loss: 0.7408 val_acc: 0.3423\n",
      "[epoch 16] [step  30] train_loss: 0.0405 train_acc: 0.9578 val_loss: 0.7568 val_acc: 0.3356\n",
      "time: 2.0632s\n",
      "[epoch 17] [step  10] train_loss: 0.0310 train_acc: 0.9588 val_loss: 0.7632 val_acc: 0.3311\n",
      "[epoch 17] [step  20] train_loss: 0.0358 train_acc: 0.9578 val_loss: 0.7712 val_acc: 0.3209\n",
      "[epoch 17] [step  30] train_loss: 0.0318 train_acc: 0.9563 val_loss: 0.7587 val_acc: 0.3356\n",
      "time: 2.6312s\n",
      "[epoch 18] [step  10] train_loss: 0.0389 train_acc: 0.9460 val_loss: 0.7758 val_acc: 0.3300\n",
      "[epoch 18] [step  20] train_loss: 0.0347 train_acc: 0.9516 val_loss: 0.7713 val_acc: 0.3255\n",
      "[epoch 18] [step  30] train_loss: 0.0264 train_acc: 0.9734 val_loss: 0.7588 val_acc: 0.3367\n",
      "time: 2.7033s\n",
      "[epoch 19] [step  10] train_loss: 0.0229 train_acc: 0.9730 val_loss: 0.7604 val_acc: 0.3277\n",
      "[epoch 19] [step  20] train_loss: 0.0403 train_acc: 0.9469 val_loss: 0.7668 val_acc: 0.3288\n",
      "[epoch 19] [step  30] train_loss: 0.0378 train_acc: 0.9547 val_loss: 0.7530 val_acc: 0.3378\n",
      "time: 2.4476s\n",
      "max_val_accuracy: 0.3547297297297297\n"
     ]
    }
   ],
   "source": [
    "#torch.backends.cudnn.benchmark = True\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now For The Testing Of This Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RecurrentCapsuleNetwork:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([6615, 300]) from checkpoint, the shape in current model is torch.Size([3416, 300]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39m# Print The Accuracy Relative to the test data.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest:\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39maccuracy: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (test_accuracy))\n\u001b[1;32m---> 32\u001b[0m test(config)\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     18\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(config[\u001b[39m'\u001b[39m\u001b[39mbase_path\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mcheckpoints/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39maspect_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m mode \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_model\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[39m# Load It into the torch Direcotry\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[0;32m     21\u001b[0m \u001b[39m# In case of Term Data We check The Term Test.xml or the Category.xml for Category\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mterm\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RecurrentCapsuleNetwork:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([6615, 300]) from checkpoint, the shape in current model is torch.Size([3416, 300])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from train import make_aspect_term_model, make_aspect_category_model\n",
    "from train.make_data import make_term_test_data, make_category_test_data\n",
    "from train.eval import eval\n",
    "\n",
    "def test(config):\n",
    "    # Pull The Config From The  Config File\n",
    "    mode = config['mode']\n",
    "    # Based On The Config Make The Model\n",
    "    if mode == 'term':\n",
    "        model = make_aspect_term_model.make_model(config)\n",
    "    else:\n",
    "        model = make_aspect_category_model.make_model(config)\n",
    "    # Move The model to the GPU If possible\n",
    "    model = model.cuda()\n",
    "    # Get the saved model's path\n",
    "    model_path = os.path.join(config['base_path'], 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])\n",
    "    # Load It into the torch Direcotry\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # In case of Term Data We check The Term Test.xml or the Category.xml for Category\n",
    "    if mode == 'term':\n",
    "        test_loader = make_term_test_data(config)\n",
    "    else:\n",
    "        test_loader = make_category_test_data(config)\n",
    "    \n",
    "    # Call The Custom Evaluation Method To Load the method \n",
    "    test_accuracy = eval(model, test_loader)\n",
    "    # Print The Accuracy Relative to the test data.\n",
    "    print('test:\\taccuracy: %.4f' % (test_accuracy))\n",
    "\n",
    "test(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Test Our Model We Need To Process Our Sentence into Word2Vec and Index2Word Then Pass The Vector Values Into The Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check Custom Inputs -> Output We need to write custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
