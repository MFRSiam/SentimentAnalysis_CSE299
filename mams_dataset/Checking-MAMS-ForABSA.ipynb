{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In This Notebook We Explore How The Mams-For-ABSA is Doing Sentiment Analysis And Try To Find out The Output Variable For this Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Entry Code of Traning The Model Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "# Import Custom Training Class From The Absa Dataset\n",
    "from train.train import train\n",
    "\n",
    "# Load The Configuration From The Config YAML File\n",
    "config = yaml.safe_load(open('config.yml'))\n",
    "\n",
    "# Check The Mode There is 2 Options for the mode Term Based And Category Based\n",
    "mode = config['mode']\n",
    "\n",
    "# Enable GPU For Traning The Model \n",
    "# This Code Forces The Pytorch Library To Use only 1 GPU \n",
    "# As Many systems have multiple gpus including my laptop\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['gpu'])\n",
    "# This Basically Pulls Out The GPU Value From The Config Value\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "#The Main Traning Code\n",
    "## train(config) -> This is The Main Function Explained Later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now If We Look Into This Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from train import make_aspect_term_model, make_aspect_category_model\n",
    "from train.make_data import make_term_data, make_category_data\n",
    "from train.make_optimizer import make_optimizer\n",
    "from train.eval import eval\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from src.module.utils.loss import CapsuleLoss\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def train(config):\n",
    "    # Check Which Mode The Model's Is in\n",
    "    mode = config['mode']\n",
    "    # Depending on the model type, it creates the model and loads the training and validation data.\n",
    "    if mode == 'term':\n",
    "        # The make_model and make_term_data function is explored later\n",
    "        model = make_aspect_term_model.make_model(config)\n",
    "        train_loader, val_loader = make_term_data(config)\n",
    "    else:\n",
    "        # The makde_model and the make_category_data is explored later\n",
    "        model = make_aspect_category_model.make_model(config)\n",
    "        train_loader, val_loader = make_category_data(config)\n",
    "    # The model is moved to the GPU using model.cuda().\n",
    "    model = model.cuda()\n",
    "    \n",
    "    base_path = config['base_path']\n",
    "    # The model's path is set using model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type']).\n",
    "    model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    # The index2word dictionary is loaded from disk using with\n",
    "    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'rb') as handle:\n",
    "        index2word = pickle.load(handle)\n",
    "    \n",
    "    # The CapsuleLoss function is instantiated using criterion = CapsuleLoss().\n",
    "    criterion = CapsuleLoss()\n",
    "    # An optimizer is created for the model using optimizer = make_optimizer(config, model).\n",
    "    optimizer = make_optimizer(config, model)\n",
    "    # The max_val_accuracy and min_val_loss variables are set to 0 and 100, respectively.\n",
    "    max_val_accuracy = 0\n",
    "    min_val_loss = 100\n",
    "    global_step = 0\n",
    "    config = config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]\n",
    "    # A loop is run for config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['num_epoches'] epochs. \n",
    "    # For each epoch, the training data is looped over and the model is trained using \n",
    "    # stochastic gradient descent. \n",
    "    # After each epoch, the model is evaluated on the validation data. \n",
    "    # The training and validation loss and accuracy are printed.\n",
    "    for epoch in range(config['num_epoches']):\n",
    "        # Initialize the total loss for the epoch to 0.\n",
    "        # Initialize the total number of samples seen in the epoch to 0.\n",
    "        # Initialize the total number of correct predictions in the epoch to 0.\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_samples = 0\n",
    "        start = time.time() #  Start a timer to measure the time taken for the epoch.\n",
    "        # This loop iterates over the batches in the training data.\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #  Increment the global step count by 1.\n",
    "            global_step += 1\n",
    "            # Set the model to training mode.\n",
    "            model.train()\n",
    "            # Extract the inputs and labels for the current batch.\n",
    "            input0, input1, label = data\n",
    "            # Move the inputs and labels to the GPU if available.\n",
    "            input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()\n",
    "            # Reset the gradients to zero.\n",
    "            optimizer.zero_grad()\n",
    "            #  Forward pass the inputs through the model to get the predictions.\n",
    "            logit = model(input0, input1)\n",
    "            # Calculate the loss based on the predictions and labels.\n",
    "            loss = criterion(logit, label)\n",
    "            # Get the size of the current batch.\n",
    "            batch_size = input0.size(0)\n",
    "            # Update the total loss for the epoch.\n",
    "            total_loss += batch_size * loss.item()\n",
    "            #  Update the total number of samples seen in the epoch.\n",
    "            total_samples += batch_size\n",
    "            # Get the predicted labels based on the highest predicted probability.\n",
    "            pred = logit.argmax(dim=1)\n",
    "            # Update the total number of correct predictions in the epoch.\n",
    "            correct_samples += (label == pred).long().sum().item()\n",
    "            # Backpropagate the loss to compute the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the gradients to avoid exploding gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            # Update the model parameters based on the computed gradients.\n",
    "            optimizer.step()\n",
    "            # Every 10 batches (except for the first batch), perform the following steps:\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                # Calculate the average loss for the epoch so far.\n",
    "                train_loss = total_loss / total_samples\n",
    "                #  Calculate the accuracy for the epoch so far.\n",
    "                train_accuracy = correct_samples / total_samples\n",
    "                # Reset the total loss for the epoch to 0.\n",
    "                # Reset the total number of samples seen in the epoch to 0.\n",
    "                # Reset the total number of correct predictions in the epoch to 0.\n",
    "                total_loss = 0\n",
    "                total_samples = 0\n",
    "                correct_samples = 0\n",
    "                # Perform validation on the validation dataset and get the validation accuracy and loss.\n",
    "                val_accuracy, val_loss = eval(model, val_loader, criterion)\n",
    "                # Print out the training and validation loss and accuracy for the current batch.\n",
    "                print('[epoch %2d] [step %3d] train_loss: %.4f train_acc: %.4f val_loss: %.4f val_acc: %.4f'\n",
    "                      % (epoch, i, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "                if val_accuracy > max_val_accuracy:\n",
    "                    max_val_accuracy = val_accuracy\n",
    "                    # torch.save(aspect_term_model.state_dict(), model_path)\n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss = val_loss\n",
    "                    if epoch > 0:\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "        end = time.time()\n",
    "        print('time: %.4fs' % (end - start))\n",
    "    print('max_val_accuracy:', max_val_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now Trying To Train The Model Using CUDNN Backend Allowing GPU Based Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  0] [step  10] train_loss: 0.3580 train_acc: 0.4190 val_loss: 0.3715 val_acc: 0.4535\n",
      "[epoch  0] [step  20] train_loss: 0.3404 train_acc: 0.4437 val_loss: 0.3478 val_acc: 0.4535\n",
      "[epoch  0] [step  30] train_loss: 0.3323 train_acc: 0.4547 val_loss: 0.3250 val_acc: 0.5480\n",
      "[epoch  0] [step  40] train_loss: 0.3059 train_acc: 0.5750 val_loss: 0.3189 val_acc: 0.5398\n",
      "[epoch  0] [step  50] train_loss: 0.3014 train_acc: 0.5500 val_loss: 0.3099 val_acc: 0.5556\n",
      "[epoch  0] [step  60] train_loss: 0.2872 train_acc: 0.5609 val_loss: 0.2888 val_acc: 0.5863\n",
      "[epoch  0] [step  70] train_loss: 0.2814 train_acc: 0.5797 val_loss: 0.2816 val_acc: 0.5916\n",
      "[epoch  0] [step  80] train_loss: 0.2737 train_acc: 0.5969 val_loss: 0.3002 val_acc: 0.5856\n",
      "[epoch  0] [step  90] train_loss: 0.2823 train_acc: 0.5531 val_loss: 0.2772 val_acc: 0.6171\n",
      "[epoch  0] [step 100] train_loss: 0.2692 train_acc: 0.6062 val_loss: 0.2824 val_acc: 0.6119\n",
      "[epoch  0] [step 110] train_loss: 0.2612 train_acc: 0.6141 val_loss: 0.2889 val_acc: 0.6216\n",
      "[epoch  0] [step 120] train_loss: 0.2622 train_acc: 0.6125 val_loss: 0.2833 val_acc: 0.6201\n",
      "[epoch  0] [step 130] train_loss: 0.2745 train_acc: 0.5875 val_loss: 0.2760 val_acc: 0.6209\n",
      "[epoch  0] [step 140] train_loss: 0.2665 train_acc: 0.6078 val_loss: 0.2718 val_acc: 0.6164\n",
      "[epoch  0] [step 150] train_loss: 0.2621 train_acc: 0.5922 val_loss: 0.2712 val_acc: 0.6194\n",
      "[epoch  0] [step 160] train_loss: 0.2579 train_acc: 0.6062 val_loss: 0.2691 val_acc: 0.6239\n",
      "[epoch  0] [step 170] train_loss: 0.2731 train_acc: 0.5781 val_loss: 0.2763 val_acc: 0.6201\n",
      "time: 12.0827s\n",
      "[epoch  1] [step  10] train_loss: 0.2552 train_acc: 0.6165 val_loss: 0.2702 val_acc: 0.6344\n",
      "[epoch  1] [step  20] train_loss: 0.2444 train_acc: 0.6391 val_loss: 0.2784 val_acc: 0.6216\n",
      "[epoch  1] [step  30] train_loss: 0.2625 train_acc: 0.6047 val_loss: 0.2672 val_acc: 0.6351\n",
      "[epoch  1] [step  40] train_loss: 0.2621 train_acc: 0.5875 val_loss: 0.2556 val_acc: 0.6336\n",
      "[epoch  1] [step  50] train_loss: 0.2469 train_acc: 0.5938 val_loss: 0.2635 val_acc: 0.6381\n",
      "[epoch  1] [step  60] train_loss: 0.2484 train_acc: 0.6188 val_loss: 0.2738 val_acc: 0.6291\n",
      "[epoch  1] [step  70] train_loss: 0.2520 train_acc: 0.6188 val_loss: 0.2665 val_acc: 0.6471\n",
      "[epoch  1] [step  80] train_loss: 0.2482 train_acc: 0.6422 val_loss: 0.2616 val_acc: 0.6449\n",
      "[epoch  1] [step  90] train_loss: 0.2618 train_acc: 0.6156 val_loss: 0.2546 val_acc: 0.6562\n",
      "[epoch  1] [step 100] train_loss: 0.2412 train_acc: 0.6547 val_loss: 0.2774 val_acc: 0.6171\n",
      "[epoch  1] [step 110] train_loss: 0.2545 train_acc: 0.6281 val_loss: 0.2634 val_acc: 0.6539\n",
      "[epoch  1] [step 120] train_loss: 0.2587 train_acc: 0.6109 val_loss: 0.2529 val_acc: 0.6539\n",
      "[epoch  1] [step 130] train_loss: 0.2631 train_acc: 0.5781 val_loss: 0.2520 val_acc: 0.6584\n",
      "[epoch  1] [step 140] train_loss: 0.2541 train_acc: 0.6281 val_loss: 0.2645 val_acc: 0.6547\n",
      "[epoch  1] [step 150] train_loss: 0.2455 train_acc: 0.6406 val_loss: 0.2578 val_acc: 0.6577\n",
      "[epoch  1] [step 160] train_loss: 0.2432 train_acc: 0.6687 val_loss: 0.2656 val_acc: 0.6404\n",
      "[epoch  1] [step 170] train_loss: 0.2571 train_acc: 0.6219 val_loss: 0.2583 val_acc: 0.6584\n",
      "time: 11.8004s\n",
      "[epoch  2] [step  10] train_loss: 0.2488 train_acc: 0.6293 val_loss: 0.2486 val_acc: 0.6637\n",
      "[epoch  2] [step  20] train_loss: 0.2336 train_acc: 0.6594 val_loss: 0.2537 val_acc: 0.6614\n",
      "[epoch  2] [step  30] train_loss: 0.2441 train_acc: 0.6219 val_loss: 0.2625 val_acc: 0.6471\n",
      "[epoch  2] [step  40] train_loss: 0.2369 train_acc: 0.6359 val_loss: 0.2509 val_acc: 0.6584\n",
      "[epoch  2] [step  50] train_loss: 0.2365 train_acc: 0.6469 val_loss: 0.2564 val_acc: 0.6517\n",
      "[epoch  2] [step  60] train_loss: 0.2280 train_acc: 0.6578 val_loss: 0.2605 val_acc: 0.6502\n",
      "[epoch  2] [step  70] train_loss: 0.2425 train_acc: 0.6438 val_loss: 0.2548 val_acc: 0.6629\n",
      "[epoch  2] [step  80] train_loss: 0.2419 train_acc: 0.6469 val_loss: 0.2513 val_acc: 0.6629\n",
      "[epoch  2] [step  90] train_loss: 0.2452 train_acc: 0.6453 val_loss: 0.2562 val_acc: 0.6486\n",
      "[epoch  2] [step 100] train_loss: 0.2362 train_acc: 0.6484 val_loss: 0.2485 val_acc: 0.6719\n",
      "[epoch  2] [step 110] train_loss: 0.2252 train_acc: 0.6672 val_loss: 0.2510 val_acc: 0.6614\n",
      "[epoch  2] [step 120] train_loss: 0.2471 train_acc: 0.6484 val_loss: 0.2463 val_acc: 0.6644\n",
      "[epoch  2] [step 130] train_loss: 0.2447 train_acc: 0.6578 val_loss: 0.2464 val_acc: 0.6652\n",
      "[epoch  2] [step 140] train_loss: 0.2330 train_acc: 0.6594 val_loss: 0.2438 val_acc: 0.6667\n",
      "[epoch  2] [step 150] train_loss: 0.2364 train_acc: 0.6234 val_loss: 0.2463 val_acc: 0.6637\n",
      "[epoch  2] [step 160] train_loss: 0.2441 train_acc: 0.6406 val_loss: 0.2392 val_acc: 0.6764\n",
      "[epoch  2] [step 170] train_loss: 0.2320 train_acc: 0.6438 val_loss: 0.2496 val_acc: 0.6629\n",
      "time: 11.5968s\n",
      "[epoch  3] [step  10] train_loss: 0.2276 train_acc: 0.6619 val_loss: 0.2436 val_acc: 0.6674\n",
      "[epoch  3] [step  20] train_loss: 0.2322 train_acc: 0.6484 val_loss: 0.2384 val_acc: 0.6742\n",
      "[epoch  3] [step  30] train_loss: 0.2251 train_acc: 0.6750 val_loss: 0.2443 val_acc: 0.6794\n",
      "[epoch  3] [step  40] train_loss: 0.2163 train_acc: 0.6797 val_loss: 0.2508 val_acc: 0.6599\n",
      "[epoch  3] [step  50] train_loss: 0.2209 train_acc: 0.6656 val_loss: 0.2541 val_acc: 0.6629\n",
      "[epoch  3] [step  60] train_loss: 0.2270 train_acc: 0.6672 val_loss: 0.2482 val_acc: 0.6704\n",
      "[epoch  3] [step  70] train_loss: 0.2300 train_acc: 0.6609 val_loss: 0.2434 val_acc: 0.6727\n",
      "[epoch  3] [step  80] train_loss: 0.2314 train_acc: 0.6500 val_loss: 0.2400 val_acc: 0.6802\n",
      "[epoch  3] [step  90] train_loss: 0.2143 train_acc: 0.7031 val_loss: 0.2434 val_acc: 0.6847\n",
      "[epoch  3] [step 100] train_loss: 0.2277 train_acc: 0.6781 val_loss: 0.2473 val_acc: 0.6704\n",
      "[epoch  3] [step 110] train_loss: 0.2268 train_acc: 0.6547 val_loss: 0.2390 val_acc: 0.6794\n",
      "[epoch  3] [step 120] train_loss: 0.2292 train_acc: 0.6641 val_loss: 0.2415 val_acc: 0.6682\n",
      "[epoch  3] [step 130] train_loss: 0.2331 train_acc: 0.6547 val_loss: 0.2482 val_acc: 0.6667\n",
      "[epoch  3] [step 140] train_loss: 0.2252 train_acc: 0.6719 val_loss: 0.2386 val_acc: 0.6749\n",
      "[epoch  3] [step 150] train_loss: 0.2116 train_acc: 0.6937 val_loss: 0.2487 val_acc: 0.6659\n",
      "[epoch  3] [step 160] train_loss: 0.2204 train_acc: 0.6750 val_loss: 0.2440 val_acc: 0.6802\n",
      "[epoch  3] [step 170] train_loss: 0.2150 train_acc: 0.6922 val_loss: 0.2448 val_acc: 0.6749\n",
      "time: 11.4173s\n",
      "[epoch  4] [step  10] train_loss: 0.2251 train_acc: 0.6705 val_loss: 0.2530 val_acc: 0.6652\n",
      "[epoch  4] [step  20] train_loss: 0.2269 train_acc: 0.6703 val_loss: 0.2424 val_acc: 0.6659\n",
      "[epoch  4] [step  30] train_loss: 0.2061 train_acc: 0.7063 val_loss: 0.2398 val_acc: 0.6794\n",
      "[epoch  4] [step  40] train_loss: 0.2197 train_acc: 0.6859 val_loss: 0.2360 val_acc: 0.6832\n",
      "[epoch  4] [step  50] train_loss: 0.2159 train_acc: 0.6859 val_loss: 0.2375 val_acc: 0.6817\n",
      "[epoch  4] [step  60] train_loss: 0.2196 train_acc: 0.6844 val_loss: 0.2379 val_acc: 0.6757\n",
      "[epoch  4] [step  70] train_loss: 0.1964 train_acc: 0.7234 val_loss: 0.2383 val_acc: 0.6817\n",
      "[epoch  4] [step  80] train_loss: 0.2196 train_acc: 0.6828 val_loss: 0.2522 val_acc: 0.6667\n",
      "[epoch  4] [step  90] train_loss: 0.2211 train_acc: 0.6797 val_loss: 0.2407 val_acc: 0.6779\n",
      "[epoch  4] [step 100] train_loss: 0.2070 train_acc: 0.7016 val_loss: 0.2475 val_acc: 0.6637\n",
      "[epoch  4] [step 110] train_loss: 0.2047 train_acc: 0.6953 val_loss: 0.2501 val_acc: 0.6644\n",
      "[epoch  4] [step 120] train_loss: 0.2250 train_acc: 0.6844 val_loss: 0.2499 val_acc: 0.6569\n",
      "[epoch  4] [step 130] train_loss: 0.2187 train_acc: 0.6703 val_loss: 0.2558 val_acc: 0.6509\n",
      "[epoch  4] [step 140] train_loss: 0.2043 train_acc: 0.7047 val_loss: 0.2536 val_acc: 0.6682\n",
      "[epoch  4] [step 150] train_loss: 0.2018 train_acc: 0.7203 val_loss: 0.2529 val_acc: 0.6697\n",
      "[epoch  4] [step 160] train_loss: 0.2014 train_acc: 0.7250 val_loss: 0.2562 val_acc: 0.6599\n",
      "[epoch  4] [step 170] train_loss: 0.1959 train_acc: 0.7172 val_loss: 0.2398 val_acc: 0.6914\n",
      "time: 11.2366s\n",
      "[epoch  5] [step  10] train_loss: 0.1960 train_acc: 0.7230 val_loss: 0.2526 val_acc: 0.6637\n",
      "[epoch  5] [step  20] train_loss: 0.1970 train_acc: 0.7188 val_loss: 0.2389 val_acc: 0.7050\n",
      "[epoch  5] [step  30] train_loss: 0.2018 train_acc: 0.7016 val_loss: 0.2386 val_acc: 0.7012\n",
      "[epoch  5] [step  40] train_loss: 0.2087 train_acc: 0.7125 val_loss: 0.2447 val_acc: 0.6764\n",
      "[epoch  5] [step  50] train_loss: 0.2033 train_acc: 0.7109 val_loss: 0.2411 val_acc: 0.6899\n",
      "[epoch  5] [step  60] train_loss: 0.1925 train_acc: 0.7234 val_loss: 0.2441 val_acc: 0.6839\n",
      "[epoch  5] [step  70] train_loss: 0.1864 train_acc: 0.7422 val_loss: 0.2398 val_acc: 0.6959\n",
      "[epoch  5] [step  80] train_loss: 0.1716 train_acc: 0.7734 val_loss: 0.2304 val_acc: 0.6937\n",
      "[epoch  5] [step  90] train_loss: 0.1855 train_acc: 0.7406 val_loss: 0.2376 val_acc: 0.6959\n",
      "[epoch  5] [step 100] train_loss: 0.1886 train_acc: 0.7328 val_loss: 0.2315 val_acc: 0.7072\n",
      "[epoch  5] [step 110] train_loss: 0.2021 train_acc: 0.7016 val_loss: 0.2297 val_acc: 0.7057\n",
      "[epoch  5] [step 120] train_loss: 0.1776 train_acc: 0.7656 val_loss: 0.2261 val_acc: 0.7027\n",
      "[epoch  5] [step 130] train_loss: 0.1868 train_acc: 0.7469 val_loss: 0.2289 val_acc: 0.7042\n",
      "[epoch  5] [step 140] train_loss: 0.1967 train_acc: 0.7219 val_loss: 0.2323 val_acc: 0.7012\n",
      "[epoch  5] [step 150] train_loss: 0.1742 train_acc: 0.7438 val_loss: 0.2346 val_acc: 0.6869\n",
      "[epoch  5] [step 160] train_loss: 0.1757 train_acc: 0.7609 val_loss: 0.2360 val_acc: 0.6967\n",
      "[epoch  5] [step 170] train_loss: 0.1881 train_acc: 0.7516 val_loss: 0.2351 val_acc: 0.6989\n",
      "time: 11.3480s\n",
      "[epoch  6] [step  10] train_loss: 0.1674 train_acc: 0.7685 val_loss: 0.2274 val_acc: 0.7087\n",
      "[epoch  6] [step  20] train_loss: 0.1662 train_acc: 0.7875 val_loss: 0.2324 val_acc: 0.7005\n",
      "[epoch  6] [step  30] train_loss: 0.1702 train_acc: 0.7750 val_loss: 0.2336 val_acc: 0.6982\n",
      "[epoch  6] [step  40] train_loss: 0.1727 train_acc: 0.7594 val_loss: 0.2369 val_acc: 0.7012\n",
      "[epoch  6] [step  50] train_loss: 0.1666 train_acc: 0.7781 val_loss: 0.2321 val_acc: 0.7095\n",
      "[epoch  6] [step  60] train_loss: 0.1750 train_acc: 0.7562 val_loss: 0.2275 val_acc: 0.7102\n",
      "[epoch  6] [step  70] train_loss: 0.1648 train_acc: 0.7891 val_loss: 0.2263 val_acc: 0.7042\n",
      "[epoch  6] [step  80] train_loss: 0.1691 train_acc: 0.7609 val_loss: 0.2243 val_acc: 0.7200\n",
      "[epoch  6] [step  90] train_loss: 0.1739 train_acc: 0.7547 val_loss: 0.2239 val_acc: 0.7140\n",
      "[epoch  6] [step 100] train_loss: 0.1793 train_acc: 0.7672 val_loss: 0.2266 val_acc: 0.7222\n",
      "[epoch  6] [step 110] train_loss: 0.1710 train_acc: 0.7719 val_loss: 0.2184 val_acc: 0.7222\n",
      "[epoch  6] [step 120] train_loss: 0.1691 train_acc: 0.7641 val_loss: 0.2137 val_acc: 0.7320\n",
      "[epoch  6] [step 130] train_loss: 0.1726 train_acc: 0.7609 val_loss: 0.2146 val_acc: 0.7260\n",
      "[epoch  6] [step 140] train_loss: 0.1608 train_acc: 0.7922 val_loss: 0.2122 val_acc: 0.7387\n",
      "[epoch  6] [step 150] train_loss: 0.1487 train_acc: 0.8047 val_loss: 0.2167 val_acc: 0.7312\n",
      "[epoch  6] [step 160] train_loss: 0.1589 train_acc: 0.7906 val_loss: 0.2177 val_acc: 0.7282\n",
      "[epoch  6] [step 170] train_loss: 0.1460 train_acc: 0.7937 val_loss: 0.2172 val_acc: 0.7267\n",
      "time: 11.6235s\n",
      "[epoch  7] [step  10] train_loss: 0.1532 train_acc: 0.8054 val_loss: 0.2209 val_acc: 0.7230\n",
      "[epoch  7] [step  20] train_loss: 0.1517 train_acc: 0.7859 val_loss: 0.2159 val_acc: 0.7335\n",
      "[epoch  7] [step  30] train_loss: 0.1467 train_acc: 0.8078 val_loss: 0.2145 val_acc: 0.7372\n",
      "[epoch  7] [step  40] train_loss: 0.1600 train_acc: 0.7859 val_loss: 0.2209 val_acc: 0.7162\n",
      "[epoch  7] [step  50] train_loss: 0.1495 train_acc: 0.7953 val_loss: 0.2131 val_acc: 0.7275\n",
      "[epoch  7] [step  60] train_loss: 0.1511 train_acc: 0.7969 val_loss: 0.2118 val_acc: 0.7357\n",
      "[epoch  7] [step  70] train_loss: 0.1502 train_acc: 0.7906 val_loss: 0.2085 val_acc: 0.7372\n",
      "[epoch  7] [step  80] train_loss: 0.1368 train_acc: 0.8250 val_loss: 0.2032 val_acc: 0.7417\n",
      "[epoch  7] [step  90] train_loss: 0.1552 train_acc: 0.7828 val_loss: 0.2094 val_acc: 0.7342\n",
      "[epoch  7] [step 100] train_loss: 0.1579 train_acc: 0.7812 val_loss: 0.2107 val_acc: 0.7402\n",
      "[epoch  7] [step 110] train_loss: 0.1493 train_acc: 0.8000 val_loss: 0.2146 val_acc: 0.7335\n",
      "[epoch  7] [step 120] train_loss: 0.1601 train_acc: 0.7766 val_loss: 0.2109 val_acc: 0.7425\n",
      "[epoch  7] [step 130] train_loss: 0.1610 train_acc: 0.7891 val_loss: 0.2069 val_acc: 0.7515\n",
      "[epoch  7] [step 140] train_loss: 0.1298 train_acc: 0.8219 val_loss: 0.2058 val_acc: 0.7380\n",
      "[epoch  7] [step 150] train_loss: 0.1453 train_acc: 0.8031 val_loss: 0.2051 val_acc: 0.7342\n",
      "[epoch  7] [step 160] train_loss: 0.1571 train_acc: 0.7922 val_loss: 0.2021 val_acc: 0.7455\n",
      "[epoch  7] [step 170] train_loss: 0.1581 train_acc: 0.7984 val_loss: 0.2145 val_acc: 0.7387\n",
      "time: 11.3456s\n",
      "[epoch  8] [step  10] train_loss: 0.1427 train_acc: 0.7997 val_loss: 0.2062 val_acc: 0.7485\n",
      "[epoch  8] [step  20] train_loss: 0.1408 train_acc: 0.8281 val_loss: 0.2083 val_acc: 0.7432\n",
      "[epoch  8] [step  30] train_loss: 0.1446 train_acc: 0.8000 val_loss: 0.2056 val_acc: 0.7410\n",
      "[epoch  8] [step  40] train_loss: 0.1544 train_acc: 0.7969 val_loss: 0.2014 val_acc: 0.7590\n",
      "[epoch  8] [step  50] train_loss: 0.1195 train_acc: 0.8391 val_loss: 0.2041 val_acc: 0.7553\n",
      "[epoch  8] [step  60] train_loss: 0.1344 train_acc: 0.8172 val_loss: 0.2036 val_acc: 0.7560\n",
      "[epoch  8] [step  70] train_loss: 0.1342 train_acc: 0.8172 val_loss: 0.2034 val_acc: 0.7515\n",
      "[epoch  8] [step  80] train_loss: 0.1391 train_acc: 0.8047 val_loss: 0.1994 val_acc: 0.7553\n",
      "[epoch  8] [step  90] train_loss: 0.1305 train_acc: 0.8359 val_loss: 0.2072 val_acc: 0.7500\n",
      "[epoch  8] [step 100] train_loss: 0.1251 train_acc: 0.8359 val_loss: 0.2077 val_acc: 0.7447\n",
      "[epoch  8] [step 110] train_loss: 0.1419 train_acc: 0.8156 val_loss: 0.2163 val_acc: 0.7365\n",
      "[epoch  8] [step 120] train_loss: 0.1400 train_acc: 0.8047 val_loss: 0.2053 val_acc: 0.7470\n",
      "[epoch  8] [step 130] train_loss: 0.1529 train_acc: 0.7844 val_loss: 0.2018 val_acc: 0.7530\n",
      "[epoch  8] [step 140] train_loss: 0.1257 train_acc: 0.8344 val_loss: 0.2035 val_acc: 0.7410\n",
      "[epoch  8] [step 150] train_loss: 0.1349 train_acc: 0.8266 val_loss: 0.2128 val_acc: 0.7357\n",
      "[epoch  8] [step 160] train_loss: 0.1528 train_acc: 0.7969 val_loss: 0.2103 val_acc: 0.7312\n",
      "[epoch  8] [step 170] train_loss: 0.1433 train_acc: 0.8016 val_loss: 0.2081 val_acc: 0.7380\n",
      "time: 11.3425s\n",
      "[epoch  9] [step  10] train_loss: 0.1383 train_acc: 0.8239 val_loss: 0.2090 val_acc: 0.7425\n",
      "[epoch  9] [step  20] train_loss: 0.1462 train_acc: 0.8109 val_loss: 0.2076 val_acc: 0.7620\n",
      "[epoch  9] [step  30] train_loss: 0.1363 train_acc: 0.8203 val_loss: 0.2108 val_acc: 0.7470\n",
      "[epoch  9] [step  40] train_loss: 0.1396 train_acc: 0.8156 val_loss: 0.2070 val_acc: 0.7492\n",
      "[epoch  9] [step  50] train_loss: 0.1145 train_acc: 0.8469 val_loss: 0.2088 val_acc: 0.7417\n",
      "[epoch  9] [step  60] train_loss: 0.1332 train_acc: 0.8266 val_loss: 0.2078 val_acc: 0.7425\n",
      "[epoch  9] [step  70] train_loss: 0.1101 train_acc: 0.8641 val_loss: 0.2170 val_acc: 0.7305\n",
      "[epoch  9] [step  80] train_loss: 0.1045 train_acc: 0.8672 val_loss: 0.2082 val_acc: 0.7440\n",
      "[epoch  9] [step  90] train_loss: 0.1336 train_acc: 0.8313 val_loss: 0.2127 val_acc: 0.7260\n",
      "[epoch  9] [step 100] train_loss: 0.1362 train_acc: 0.8219 val_loss: 0.2091 val_acc: 0.7447\n",
      "[epoch  9] [step 110] train_loss: 0.1177 train_acc: 0.8484 val_loss: 0.2091 val_acc: 0.7387\n",
      "[epoch  9] [step 120] train_loss: 0.1309 train_acc: 0.8313 val_loss: 0.2075 val_acc: 0.7395\n",
      "[epoch  9] [step 130] train_loss: 0.1340 train_acc: 0.8219 val_loss: 0.2021 val_acc: 0.7470\n",
      "[epoch  9] [step 140] train_loss: 0.1482 train_acc: 0.8047 val_loss: 0.1981 val_acc: 0.7515\n",
      "[epoch  9] [step 150] train_loss: 0.1312 train_acc: 0.8172 val_loss: 0.2040 val_acc: 0.7515\n",
      "[epoch  9] [step 160] train_loss: 0.1102 train_acc: 0.8500 val_loss: 0.2044 val_acc: 0.7417\n",
      "[epoch  9] [step 170] train_loss: 0.1159 train_acc: 0.8328 val_loss: 0.2116 val_acc: 0.7387\n",
      "time: 11.3929s\n",
      "[epoch 10] [step  10] train_loss: 0.1087 train_acc: 0.8551 val_loss: 0.2118 val_acc: 0.7508\n",
      "[epoch 10] [step  20] train_loss: 0.1141 train_acc: 0.8469 val_loss: 0.2039 val_acc: 0.7500\n",
      "[epoch 10] [step  30] train_loss: 0.1264 train_acc: 0.8313 val_loss: 0.2039 val_acc: 0.7492\n",
      "[epoch 10] [step  40] train_loss: 0.1197 train_acc: 0.8375 val_loss: 0.2152 val_acc: 0.7425\n",
      "[epoch 10] [step  50] train_loss: 0.1334 train_acc: 0.8234 val_loss: 0.2051 val_acc: 0.7432\n",
      "[epoch 10] [step  60] train_loss: 0.1153 train_acc: 0.8438 val_loss: 0.2082 val_acc: 0.7477\n",
      "[epoch 10] [step  70] train_loss: 0.1122 train_acc: 0.8453 val_loss: 0.2111 val_acc: 0.7462\n",
      "[epoch 10] [step  80] train_loss: 0.1243 train_acc: 0.8438 val_loss: 0.2083 val_acc: 0.7470\n",
      "[epoch 10] [step  90] train_loss: 0.1210 train_acc: 0.8469 val_loss: 0.2088 val_acc: 0.7523\n",
      "[epoch 10] [step 100] train_loss: 0.1082 train_acc: 0.8531 val_loss: 0.2156 val_acc: 0.7417\n",
      "[epoch 10] [step 110] train_loss: 0.1144 train_acc: 0.8344 val_loss: 0.2084 val_acc: 0.7432\n",
      "[epoch 10] [step 120] train_loss: 0.1098 train_acc: 0.8641 val_loss: 0.2153 val_acc: 0.7395\n",
      "[epoch 10] [step 130] train_loss: 0.1333 train_acc: 0.8125 val_loss: 0.2070 val_acc: 0.7455\n",
      "[epoch 10] [step 140] train_loss: 0.1215 train_acc: 0.8391 val_loss: 0.2046 val_acc: 0.7462\n",
      "[epoch 10] [step 150] train_loss: 0.1169 train_acc: 0.8547 val_loss: 0.2095 val_acc: 0.7447\n",
      "[epoch 10] [step 160] train_loss: 0.1129 train_acc: 0.8594 val_loss: 0.2111 val_acc: 0.7485\n",
      "[epoch 10] [step 170] train_loss: 0.1277 train_acc: 0.8234 val_loss: 0.2128 val_acc: 0.7425\n",
      "time: 11.4458s\n",
      "[epoch 11] [step  10] train_loss: 0.1052 train_acc: 0.8665 val_loss: 0.2062 val_acc: 0.7508\n",
      "[epoch 11] [step  20] train_loss: 0.0997 train_acc: 0.8656 val_loss: 0.2115 val_acc: 0.7440\n",
      "[epoch 11] [step  30] train_loss: 0.1069 train_acc: 0.8641 val_loss: 0.2137 val_acc: 0.7515\n",
      "[epoch 11] [step  40] train_loss: 0.1161 train_acc: 0.8547 val_loss: 0.2132 val_acc: 0.7515\n",
      "[epoch 11] [step  50] train_loss: 0.1222 train_acc: 0.8297 val_loss: 0.2106 val_acc: 0.7462\n",
      "[epoch 11] [step  60] train_loss: 0.1031 train_acc: 0.8672 val_loss: 0.2082 val_acc: 0.7387\n",
      "[epoch 11] [step  70] train_loss: 0.1229 train_acc: 0.8438 val_loss: 0.2099 val_acc: 0.7350\n",
      "[epoch 11] [step  80] train_loss: 0.1046 train_acc: 0.8672 val_loss: 0.2126 val_acc: 0.7342\n",
      "[epoch 11] [step  90] train_loss: 0.1149 train_acc: 0.8375 val_loss: 0.2124 val_acc: 0.7380\n",
      "[epoch 11] [step 100] train_loss: 0.1171 train_acc: 0.8422 val_loss: 0.2121 val_acc: 0.7372\n",
      "[epoch 11] [step 110] train_loss: 0.1058 train_acc: 0.8609 val_loss: 0.2158 val_acc: 0.7380\n",
      "[epoch 11] [step 120] train_loss: 0.1174 train_acc: 0.8547 val_loss: 0.2225 val_acc: 0.7282\n",
      "[epoch 11] [step 130] train_loss: 0.1171 train_acc: 0.8453 val_loss: 0.2223 val_acc: 0.7230\n",
      "[epoch 11] [step 140] train_loss: 0.1147 train_acc: 0.8438 val_loss: 0.2168 val_acc: 0.7417\n",
      "[epoch 11] [step 150] train_loss: 0.1211 train_acc: 0.8313 val_loss: 0.2112 val_acc: 0.7402\n",
      "[epoch 11] [step 160] train_loss: 0.1115 train_acc: 0.8469 val_loss: 0.2071 val_acc: 0.7538\n",
      "[epoch 11] [step 170] train_loss: 0.1122 train_acc: 0.8484 val_loss: 0.2038 val_acc: 0.7553\n",
      "time: 11.3268s\n",
      "[epoch 12] [step  10] train_loss: 0.1035 train_acc: 0.8594 val_loss: 0.2085 val_acc: 0.7583\n",
      "[epoch 12] [step  20] train_loss: 0.1035 train_acc: 0.8656 val_loss: 0.2112 val_acc: 0.7462\n",
      "[epoch 12] [step  30] train_loss: 0.1063 train_acc: 0.8594 val_loss: 0.2064 val_acc: 0.7500\n",
      "[epoch 12] [step  40] train_loss: 0.1010 train_acc: 0.8625 val_loss: 0.2094 val_acc: 0.7470\n",
      "[epoch 12] [step  50] train_loss: 0.1063 train_acc: 0.8594 val_loss: 0.2101 val_acc: 0.7500\n",
      "[epoch 12] [step  60] train_loss: 0.1018 train_acc: 0.8578 val_loss: 0.2073 val_acc: 0.7553\n",
      "[epoch 12] [step  70] train_loss: 0.1054 train_acc: 0.8625 val_loss: 0.2094 val_acc: 0.7485\n",
      "[epoch 12] [step  80] train_loss: 0.0931 train_acc: 0.8734 val_loss: 0.2095 val_acc: 0.7462\n",
      "[epoch 12] [step  90] train_loss: 0.1082 train_acc: 0.8625 val_loss: 0.2065 val_acc: 0.7492\n",
      "[epoch 12] [step 100] train_loss: 0.0980 train_acc: 0.8734 val_loss: 0.2039 val_acc: 0.7508\n",
      "[epoch 12] [step 110] train_loss: 0.1001 train_acc: 0.8688 val_loss: 0.2045 val_acc: 0.7545\n",
      "[epoch 12] [step 120] train_loss: 0.1025 train_acc: 0.8734 val_loss: 0.2052 val_acc: 0.7598\n",
      "[epoch 12] [step 130] train_loss: 0.0943 train_acc: 0.8812 val_loss: 0.2051 val_acc: 0.7575\n",
      "[epoch 12] [step 140] train_loss: 0.1017 train_acc: 0.8719 val_loss: 0.2054 val_acc: 0.7605\n",
      "[epoch 12] [step 150] train_loss: 0.1130 train_acc: 0.8469 val_loss: 0.2072 val_acc: 0.7598\n",
      "[epoch 12] [step 160] train_loss: 0.1168 train_acc: 0.8531 val_loss: 0.2089 val_acc: 0.7553\n",
      "[epoch 12] [step 170] train_loss: 0.1085 train_acc: 0.8641 val_loss: 0.2118 val_acc: 0.7523\n",
      "time: 11.1435s\n",
      "[epoch 13] [step  10] train_loss: 0.0893 train_acc: 0.8977 val_loss: 0.2134 val_acc: 0.7538\n",
      "[epoch 13] [step  20] train_loss: 0.0767 train_acc: 0.9094 val_loss: 0.2130 val_acc: 0.7598\n",
      "[epoch 13] [step  30] train_loss: 0.0918 train_acc: 0.8891 val_loss: 0.2138 val_acc: 0.7485\n",
      "[epoch 13] [step  40] train_loss: 0.1036 train_acc: 0.8703 val_loss: 0.2120 val_acc: 0.7613\n",
      "[epoch 13] [step  50] train_loss: 0.0951 train_acc: 0.8797 val_loss: 0.2152 val_acc: 0.7462\n",
      "[epoch 13] [step  60] train_loss: 0.0907 train_acc: 0.8844 val_loss: 0.2119 val_acc: 0.7515\n",
      "[epoch 13] [step  70] train_loss: 0.0966 train_acc: 0.8828 val_loss: 0.2125 val_acc: 0.7575\n",
      "[epoch 13] [step  80] train_loss: 0.1014 train_acc: 0.8734 val_loss: 0.2155 val_acc: 0.7455\n",
      "[epoch 13] [step  90] train_loss: 0.1021 train_acc: 0.8656 val_loss: 0.2157 val_acc: 0.7538\n",
      "[epoch 13] [step 100] train_loss: 0.0963 train_acc: 0.8797 val_loss: 0.2114 val_acc: 0.7583\n",
      "[epoch 13] [step 110] train_loss: 0.0984 train_acc: 0.8750 val_loss: 0.2170 val_acc: 0.7447\n",
      "[epoch 13] [step 120] train_loss: 0.0947 train_acc: 0.8703 val_loss: 0.2182 val_acc: 0.7530\n",
      "[epoch 13] [step 130] train_loss: 0.1038 train_acc: 0.8688 val_loss: 0.2195 val_acc: 0.7477\n",
      "[epoch 13] [step 140] train_loss: 0.0956 train_acc: 0.8766 val_loss: 0.2152 val_acc: 0.7523\n",
      "[epoch 13] [step 150] train_loss: 0.1070 train_acc: 0.8625 val_loss: 0.2150 val_acc: 0.7455\n",
      "[epoch 13] [step 160] train_loss: 0.1089 train_acc: 0.8688 val_loss: 0.2105 val_acc: 0.7455\n",
      "[epoch 13] [step 170] train_loss: 0.1020 train_acc: 0.8734 val_loss: 0.2101 val_acc: 0.7440\n",
      "time: 11.3262s\n",
      "[epoch 14] [step  10] train_loss: 0.0807 train_acc: 0.9020 val_loss: 0.2087 val_acc: 0.7560\n",
      "[epoch 14] [step  20] train_loss: 0.0801 train_acc: 0.9047 val_loss: 0.2166 val_acc: 0.7455\n",
      "[epoch 14] [step  30] train_loss: 0.0843 train_acc: 0.8969 val_loss: 0.2195 val_acc: 0.7432\n",
      "[epoch 14] [step  40] train_loss: 0.1026 train_acc: 0.8672 val_loss: 0.2188 val_acc: 0.7432\n",
      "[epoch 14] [step  50] train_loss: 0.0800 train_acc: 0.8969 val_loss: 0.2215 val_acc: 0.7387\n",
      "[epoch 14] [step  60] train_loss: 0.0832 train_acc: 0.8891 val_loss: 0.2209 val_acc: 0.7410\n",
      "[epoch 14] [step  70] train_loss: 0.0892 train_acc: 0.8828 val_loss: 0.2200 val_acc: 0.7492\n",
      "[epoch 14] [step  80] train_loss: 0.0902 train_acc: 0.8859 val_loss: 0.2281 val_acc: 0.7327\n",
      "[epoch 14] [step  90] train_loss: 0.1028 train_acc: 0.8672 val_loss: 0.2215 val_acc: 0.7417\n",
      "[epoch 14] [step 100] train_loss: 0.0806 train_acc: 0.8969 val_loss: 0.2213 val_acc: 0.7402\n",
      "[epoch 14] [step 110] train_loss: 0.0887 train_acc: 0.8734 val_loss: 0.2195 val_acc: 0.7485\n",
      "[epoch 14] [step 120] train_loss: 0.0901 train_acc: 0.8875 val_loss: 0.2185 val_acc: 0.7425\n",
      "[epoch 14] [step 130] train_loss: 0.0997 train_acc: 0.8703 val_loss: 0.2103 val_acc: 0.7545\n",
      "[epoch 14] [step 140] train_loss: 0.0877 train_acc: 0.8859 val_loss: 0.2161 val_acc: 0.7477\n",
      "[epoch 14] [step 150] train_loss: 0.0783 train_acc: 0.9031 val_loss: 0.2198 val_acc: 0.7417\n",
      "[epoch 14] [step 160] train_loss: 0.0850 train_acc: 0.8953 val_loss: 0.2244 val_acc: 0.7402\n",
      "[epoch 14] [step 170] train_loss: 0.1057 train_acc: 0.8719 val_loss: 0.2306 val_acc: 0.7297\n",
      "time: 11.5513s\n",
      "[epoch 15] [step  10] train_loss: 0.0894 train_acc: 0.8849 val_loss: 0.2188 val_acc: 0.7477\n",
      "[epoch 15] [step  20] train_loss: 0.0806 train_acc: 0.8953 val_loss: 0.2155 val_acc: 0.7553\n",
      "[epoch 15] [step  30] train_loss: 0.0911 train_acc: 0.8938 val_loss: 0.2271 val_acc: 0.7350\n",
      "[epoch 15] [step  40] train_loss: 0.0761 train_acc: 0.9094 val_loss: 0.2179 val_acc: 0.7462\n",
      "[epoch 15] [step  50] train_loss: 0.0871 train_acc: 0.8938 val_loss: 0.2183 val_acc: 0.7432\n",
      "[epoch 15] [step  60] train_loss: 0.0819 train_acc: 0.8938 val_loss: 0.2248 val_acc: 0.7432\n",
      "[epoch 15] [step  70] train_loss: 0.0813 train_acc: 0.8969 val_loss: 0.2151 val_acc: 0.7568\n",
      "[epoch 15] [step  80] train_loss: 0.0854 train_acc: 0.8922 val_loss: 0.2214 val_acc: 0.7545\n",
      "[epoch 15] [step  90] train_loss: 0.0796 train_acc: 0.9078 val_loss: 0.2229 val_acc: 0.7500\n",
      "[epoch 15] [step 100] train_loss: 0.0893 train_acc: 0.8859 val_loss: 0.2239 val_acc: 0.7485\n",
      "[epoch 15] [step 110] train_loss: 0.0873 train_acc: 0.8891 val_loss: 0.2224 val_acc: 0.7485\n",
      "[epoch 15] [step 120] train_loss: 0.0959 train_acc: 0.8719 val_loss: 0.2236 val_acc: 0.7455\n",
      "[epoch 15] [step 130] train_loss: 0.0955 train_acc: 0.8672 val_loss: 0.2264 val_acc: 0.7402\n",
      "[epoch 15] [step 140] train_loss: 0.0822 train_acc: 0.9062 val_loss: 0.2184 val_acc: 0.7492\n",
      "[epoch 15] [step 150] train_loss: 0.0923 train_acc: 0.8656 val_loss: 0.2268 val_acc: 0.7432\n",
      "[epoch 15] [step 160] train_loss: 0.0756 train_acc: 0.9016 val_loss: 0.2358 val_acc: 0.7335\n",
      "[epoch 15] [step 170] train_loss: 0.0820 train_acc: 0.8984 val_loss: 0.2283 val_acc: 0.7447\n",
      "time: 11.5510s\n",
      "[epoch 16] [step  10] train_loss: 0.0781 train_acc: 0.9006 val_loss: 0.2383 val_acc: 0.7335\n",
      "[epoch 16] [step  20] train_loss: 0.0802 train_acc: 0.9062 val_loss: 0.2363 val_acc: 0.7357\n",
      "[epoch 16] [step  30] train_loss: 0.0875 train_acc: 0.8875 val_loss: 0.2283 val_acc: 0.7440\n",
      "[epoch 16] [step  40] train_loss: 0.0806 train_acc: 0.8859 val_loss: 0.2327 val_acc: 0.7350\n",
      "[epoch 16] [step  50] train_loss: 0.0676 train_acc: 0.9078 val_loss: 0.2292 val_acc: 0.7357\n",
      "[epoch 16] [step  60] train_loss: 0.0762 train_acc: 0.9062 val_loss: 0.2322 val_acc: 0.7327\n",
      "[epoch 16] [step  70] train_loss: 0.0706 train_acc: 0.9094 val_loss: 0.2374 val_acc: 0.7275\n",
      "[epoch 16] [step  80] train_loss: 0.0806 train_acc: 0.8891 val_loss: 0.2259 val_acc: 0.7440\n",
      "[epoch 16] [step  90] train_loss: 0.0811 train_acc: 0.8969 val_loss: 0.2245 val_acc: 0.7402\n",
      "[epoch 16] [step 100] train_loss: 0.0680 train_acc: 0.9156 val_loss: 0.2286 val_acc: 0.7350\n",
      "[epoch 16] [step 110] train_loss: 0.0880 train_acc: 0.8875 val_loss: 0.2314 val_acc: 0.7342\n",
      "[epoch 16] [step 120] train_loss: 0.0764 train_acc: 0.8953 val_loss: 0.2246 val_acc: 0.7477\n",
      "[epoch 16] [step 130] train_loss: 0.0888 train_acc: 0.8828 val_loss: 0.2234 val_acc: 0.7410\n",
      "[epoch 16] [step 140] train_loss: 0.0921 train_acc: 0.8891 val_loss: 0.2239 val_acc: 0.7372\n",
      "[epoch 16] [step 150] train_loss: 0.0680 train_acc: 0.8984 val_loss: 0.2223 val_acc: 0.7425\n",
      "[epoch 16] [step 160] train_loss: 0.0823 train_acc: 0.9000 val_loss: 0.2311 val_acc: 0.7357\n",
      "[epoch 16] [step 170] train_loss: 0.0811 train_acc: 0.8875 val_loss: 0.2256 val_acc: 0.7395\n",
      "time: 11.5299s\n",
      "[epoch 17] [step  10] train_loss: 0.0660 train_acc: 0.9205 val_loss: 0.2295 val_acc: 0.7380\n",
      "[epoch 17] [step  20] train_loss: 0.0698 train_acc: 0.9172 val_loss: 0.2326 val_acc: 0.7387\n",
      "[epoch 17] [step  30] train_loss: 0.0780 train_acc: 0.9047 val_loss: 0.2351 val_acc: 0.7312\n",
      "[epoch 17] [step  40] train_loss: 0.0703 train_acc: 0.9047 val_loss: 0.2348 val_acc: 0.7335\n",
      "[epoch 17] [step  50] train_loss: 0.0679 train_acc: 0.9109 val_loss: 0.2320 val_acc: 0.7485\n",
      "[epoch 17] [step  60] train_loss: 0.0649 train_acc: 0.9156 val_loss: 0.2315 val_acc: 0.7410\n",
      "[epoch 17] [step  70] train_loss: 0.0678 train_acc: 0.9078 val_loss: 0.2287 val_acc: 0.7470\n",
      "[epoch 17] [step  80] train_loss: 0.0697 train_acc: 0.9203 val_loss: 0.2333 val_acc: 0.7417\n",
      "[epoch 17] [step  90] train_loss: 0.0581 train_acc: 0.9281 val_loss: 0.2370 val_acc: 0.7432\n",
      "[epoch 17] [step 100] train_loss: 0.0599 train_acc: 0.9234 val_loss: 0.2349 val_acc: 0.7342\n",
      "[epoch 17] [step 110] train_loss: 0.0751 train_acc: 0.9000 val_loss: 0.2321 val_acc: 0.7320\n",
      "[epoch 17] [step 120] train_loss: 0.0860 train_acc: 0.8922 val_loss: 0.2287 val_acc: 0.7410\n",
      "[epoch 17] [step 130] train_loss: 0.0853 train_acc: 0.8719 val_loss: 0.2228 val_acc: 0.7508\n",
      "[epoch 17] [step 140] train_loss: 0.0729 train_acc: 0.9047 val_loss: 0.2191 val_acc: 0.7568\n",
      "[epoch 17] [step 150] train_loss: 0.0858 train_acc: 0.8906 val_loss: 0.2256 val_acc: 0.7477\n",
      "[epoch 17] [step 160] train_loss: 0.0710 train_acc: 0.9156 val_loss: 0.2200 val_acc: 0.7523\n",
      "[epoch 17] [step 170] train_loss: 0.0829 train_acc: 0.8953 val_loss: 0.2190 val_acc: 0.7500\n",
      "time: 11.3402s\n",
      "[epoch 18] [step  10] train_loss: 0.0584 train_acc: 0.9347 val_loss: 0.2288 val_acc: 0.7432\n",
      "[epoch 18] [step  20] train_loss: 0.0705 train_acc: 0.9094 val_loss: 0.2284 val_acc: 0.7387\n",
      "[epoch 18] [step  30] train_loss: 0.0575 train_acc: 0.9297 val_loss: 0.2338 val_acc: 0.7387\n",
      "[epoch 18] [step  40] train_loss: 0.0613 train_acc: 0.9203 val_loss: 0.2263 val_acc: 0.7432\n",
      "[epoch 18] [step  50] train_loss: 0.0565 train_acc: 0.9281 val_loss: 0.2295 val_acc: 0.7523\n",
      "[epoch 18] [step  60] train_loss: 0.0754 train_acc: 0.9062 val_loss: 0.2282 val_acc: 0.7568\n",
      "[epoch 18] [step  70] train_loss: 0.0755 train_acc: 0.9031 val_loss: 0.2292 val_acc: 0.7470\n",
      "[epoch 18] [step  80] train_loss: 0.0495 train_acc: 0.9344 val_loss: 0.2337 val_acc: 0.7425\n",
      "[epoch 18] [step  90] train_loss: 0.0775 train_acc: 0.9078 val_loss: 0.2348 val_acc: 0.7508\n",
      "[epoch 18] [step 100] train_loss: 0.0660 train_acc: 0.9125 val_loss: 0.2315 val_acc: 0.7500\n",
      "[epoch 18] [step 110] train_loss: 0.0778 train_acc: 0.9094 val_loss: 0.2275 val_acc: 0.7462\n",
      "[epoch 18] [step 120] train_loss: 0.0754 train_acc: 0.8938 val_loss: 0.2271 val_acc: 0.7485\n",
      "[epoch 18] [step 130] train_loss: 0.0624 train_acc: 0.9250 val_loss: 0.2219 val_acc: 0.7515\n",
      "[epoch 18] [step 140] train_loss: 0.0729 train_acc: 0.9125 val_loss: 0.2276 val_acc: 0.7500\n",
      "[epoch 18] [step 150] train_loss: 0.0822 train_acc: 0.8953 val_loss: 0.2238 val_acc: 0.7553\n",
      "[epoch 18] [step 160] train_loss: 0.0694 train_acc: 0.9109 val_loss: 0.2251 val_acc: 0.7462\n",
      "[epoch 18] [step 170] train_loss: 0.0658 train_acc: 0.9109 val_loss: 0.2273 val_acc: 0.7410\n",
      "time: 11.3845s\n",
      "[epoch 19] [step  10] train_loss: 0.0638 train_acc: 0.9261 val_loss: 0.2277 val_acc: 0.7470\n",
      "[epoch 19] [step  20] train_loss: 0.0621 train_acc: 0.9250 val_loss: 0.2350 val_acc: 0.7432\n",
      "[epoch 19] [step  30] train_loss: 0.0530 train_acc: 0.9313 val_loss: 0.2382 val_acc: 0.7350\n",
      "[epoch 19] [step  40] train_loss: 0.0561 train_acc: 0.9234 val_loss: 0.2280 val_acc: 0.7508\n",
      "[epoch 19] [step  50] train_loss: 0.0608 train_acc: 0.9234 val_loss: 0.2294 val_acc: 0.7425\n",
      "[epoch 19] [step  60] train_loss: 0.0486 train_acc: 0.9469 val_loss: 0.2377 val_acc: 0.7395\n",
      "[epoch 19] [step  70] train_loss: 0.0636 train_acc: 0.9203 val_loss: 0.2353 val_acc: 0.7417\n",
      "[epoch 19] [step  80] train_loss: 0.0600 train_acc: 0.9266 val_loss: 0.2350 val_acc: 0.7477\n",
      "[epoch 19] [step  90] train_loss: 0.0635 train_acc: 0.9219 val_loss: 0.2372 val_acc: 0.7425\n",
      "[epoch 19] [step 100] train_loss: 0.0612 train_acc: 0.9250 val_loss: 0.2397 val_acc: 0.7462\n",
      "[epoch 19] [step 110] train_loss: 0.0658 train_acc: 0.9172 val_loss: 0.2346 val_acc: 0.7455\n",
      "[epoch 19] [step 120] train_loss: 0.0741 train_acc: 0.9094 val_loss: 0.2314 val_acc: 0.7372\n",
      "[epoch 19] [step 130] train_loss: 0.0639 train_acc: 0.9219 val_loss: 0.2319 val_acc: 0.7485\n",
      "[epoch 19] [step 140] train_loss: 0.0680 train_acc: 0.9125 val_loss: 0.2350 val_acc: 0.7425\n",
      "[epoch 19] [step 150] train_loss: 0.0772 train_acc: 0.9047 val_loss: 0.2370 val_acc: 0.7357\n",
      "[epoch 19] [step 160] train_loss: 0.0668 train_acc: 0.9078 val_loss: 0.2384 val_acc: 0.7335\n",
      "[epoch 19] [step 170] train_loss: 0.0723 train_acc: 0.9078 val_loss: 0.2366 val_acc: 0.7425\n",
      "time: 11.3482s\n",
      "max_val_accuracy: 0.762012012012012\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now For The Testing Of This Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\taccuracy: 0.7455\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from train import make_aspect_term_model, make_aspect_category_model\n",
    "from train.make_data import make_term_test_data, make_category_test_data\n",
    "from train.eval import eval\n",
    "\n",
    "def test(config):\n",
    "    # Pull The Config From The  Config File\n",
    "    mode = config['mode']\n",
    "    # Based On The Config Make The Model\n",
    "    if mode == 'term':\n",
    "        model = make_aspect_term_model.make_model(config)\n",
    "    else:\n",
    "        model = make_aspect_category_model.make_model(config)\n",
    "    # Move The model to the GPU If possible\n",
    "    model = model.cuda()\n",
    "    # Get the saved model's path\n",
    "    model_path = os.path.join(config['base_path'], 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])\n",
    "    # Load It into the torch Direcotry\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # In case of Term Data We check The Term Test.xml or the Category.xml for Category\n",
    "    if mode == 'term':\n",
    "        test_loader = make_term_test_data(config)\n",
    "    else:\n",
    "        test_loader = make_category_test_data(config)\n",
    "    \n",
    "    # Call The Custom Evaluation Method To Load the method \n",
    "    test_accuracy = eval(model, test_loader)\n",
    "    # Print The Accuracy Relative to the test data.\n",
    "    print('test:\\taccuracy: %.4f' % (test_accuracy))\n",
    "\n",
    "test(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Test Our Model We Need To Process Our Sentence into Word2Vec and Index2Word Then Pass The Vector Values Into The Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check Custom Inputs -> Output We need to write custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
