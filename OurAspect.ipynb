{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "\n",
      "Default download directory: C:\\Users\\mfrfo\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: E:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\stanfordNLP\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235M/235M [00:44<00:00, 5.24MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: E:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\stanfordNLP\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mfrfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mfrfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mfrfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The Sound Quality is great but the battery life is very bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.lower()\n",
    "sentList = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for line in sentList:\n",
    "    txt_list = nltk.word_tokenize(line)\n",
    "    taggedList = nltk.pos_tag(txt_list)\n",
    "print(taggedList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the soundquality is great but the batterylife is very bad .\n"
     ]
    }
   ],
   "source": [
    "newwordList = []\n",
    "flag = 0\n",
    "for i in range(0,len(taggedList)-1):\n",
    "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "        newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "        flag=1\n",
    "    else:\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "        newwordList.append(taggedList[i][0])\n",
    "        if(i==len(taggedList)-2):\n",
    "            newwordList.append(taggedList[i+1][0])\n",
    "finaltxt = ' '.join(word for word in newwordList)\n",
    "print(finaltxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "taggedList = nltk.pos_tag(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\mfrfo\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Cannot load model from C:\\Users\\mfrfo\\stanfordnlp_resources\\en_ewt_models\\en_ewt_tokenizer.pt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:82\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(filename, \u001b[39mlambda\u001b[39;49;00m storage, loc: storage)\n\u001b[0;32m     83\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\mfrfo\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[39m=\u001b[39m stanfordnlp\u001b[39m.\u001b[39;49mPipeline()\n\u001b[0;32m      2\u001b[0m doc \u001b[39m=\u001b[39m nlp(finaltxt)\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\pipeline\\core.py:121\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[0;32m    122\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    123\u001b[0m                                                                               use_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_gpu)\n\u001b[0;32m    124\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    125\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\pipeline\\processor.py:102\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, use_gpu)\n\u001b[0;32m    103\u001b[0m \u001b[39m# run set up process\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\pipeline\\tokenize_processor.py:31\u001b[0m, in \u001b[0;36mTokenizeProcessor._set_up_model\u001b[1;34m(self, config, use_gpu)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m Trainer(model_file\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmodel_path\u001b[39;49m\u001b[39m'\u001b[39;49m], use_cuda\u001b[39m=\u001b[39;49muse_gpu)\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:16\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, args, vocab, model_file, use_cuda)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[39m# load everything from file\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# build model from scratch\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\stanfordnlp\\models\\tokenize\\trainer.py:85\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCannot load model from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(filename))\n\u001b[1;32m---> 85\u001b[0m     sys\u001b[39m.\u001b[39;49mexit(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2047\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[0;32m   2045\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2046\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 2047\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[0;32m   2048\u001b[0m                                                      value))\n\u001b[0;32m   2049\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2051\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2052\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:636\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[0;32m    629\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \n\u001b[0;32m    631\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:503\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    500\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    501\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    502\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 503\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m    504\u001b[0m             etype, evalue, (etb, chained_exc_ids),\n\u001b[0;32m    505\u001b[0m             chained_exceptions_tb_offset, context)\n\u001b[0;32m    506\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[0;32m    507\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[0;32m    509\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:1288\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1287\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m tb\n\u001b[1;32m-> 1288\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1289\u001b[0m     \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:1177\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1174\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[0;32m   1176\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1177\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1178\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[0;32m   1180\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1181\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:1030\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[0;32m   1022\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1023\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m   1028\u001b[0m ):\n\u001b[0;32m   1029\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1031\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1033\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:935\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[0;32m    933\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(etype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[0;32m    934\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[0;32m    936\u001b[0m )\n\u001b[0;32m    938\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m    939\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32me:\\Programming\\299 Project\\Sentiment Data\\Project\\SentimentAnalysis_CSE299\\env\\Lib\\site-packages\\IPython\\core\\ultratb.py:1002\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1000\u001b[0m tbs \u001b[39m=\u001b[39m []\n\u001b[0;32m   1001\u001b[0m \u001b[39mwhile\u001b[39;00m cf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1002\u001b[0m     source_file \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetsourcefile(etb\u001b[39m.\u001b[39;49mtb_frame)\n\u001b[0;32m   1003\u001b[0m     lines, first \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetsourcelines(etb\u001b[39m.\u001b[39mtb_frame)\n\u001b[0;32m   1004\u001b[0m     max_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(max_len, first \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(lines))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(models_dir='E:/Programming/299 Project/Sentiment Data/Project/SentimentAnalysis_CSE299/stanfordNLP')\n",
    "doc = nlp(finaltxt)\n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "for i in range(0, len(dep_node)):\n",
    "    if (int(dep_node[i][1]) != 0):\n",
    "        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "print(dep_node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
